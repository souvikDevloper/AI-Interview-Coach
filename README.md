# GPT Interviewer üé§
_AI‚Äëpowered mock interviews for tech roles ‚Äî voice in, voice out, instant feedback._

![Stack](https://img.shields.io/badge/Frontend-Streamlit-FF4B4B?logo=streamlit&logoColor=white)
![Framework](https://img.shields.io/badge/Framework-LangChain-1C3C3C)
![LLM](https://img.shields.io/badge/LLM-Fireworks%20AI-6C39F4)
![Search](https://img.shields.io/badge/Vector%20DB-FAISS-2E77BC)
![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)

---

## Table of contents
- [Highlights](#highlights)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Install & run](#install--run)
- [Environment config](#environment-config)
- [Usage](#usage)
- [Performance tips](#performance-tips)
- [Troubleshooting](#troubleshooting)
- [Project layout](#project-layout)
- [License](#license)

---

## Highlights

- **Three interview modes**
  - **Professional (JD‚Äëbased)** ‚Äî paste a job description; the app builds a guideline and asks one focused technical question per topic.
  - **Behavioral** ‚Äî STAR‚Äëstyle soft‚Äëskill questions with rubric‚Äëbased feedback.
  - **R√©sum√©** ‚Äî parse your PDF and drill into your actual experience.
- **Voice I/O** ‚Äî local Whisper STT + Edge‚ÄëTTS for natural conversation.
- **Retrieval‚Äëaugmented** ‚Äî Fireworks embeddings ‚Üí FAISS similarity search.
- **One‚Äëclick report** ‚Äî strengths, gaps, and an overall score you can download.

---

## Architecture

```mermaid
flowchart TD
  A[User] --> B[Streamlit UI]

  %% Input path
  B --> C{Voice enabled?}
  C -- Yes --> D[AudioRecorder]
  D --> E["Whisper\n(faster-whisper)"]
  C -- No --> F[Typed input]
  E --> F

  %% Retrieval & memory
  F --> G[LangChain Memory]
  F --> H["Retriever:\nFAISS + FireworksEmbeddings"]
  H --> I[Top-k context]

  %% Conversation
  G --> J[Conversation Chain Prompt]
  I --> J
  J --> K["ChatFireworks LLM\nllama-v3p1-8b-instruct"]
  K --> L[Interviewer reply]

  %% TTS
  L --> M{Auto-play voice?}
  M -- Yes --> N["Edge-TTS\n(audio out)"]
  M -- No --> B

  %% Feedback
  B --> O["Get feedback" button]
  O --> P[Feedback Chain]
  P --> Q["Scored report (.txt)"]

```
<details><summary>ASCII fallback (if Mermaid is disabled)</summary>

```
User -> Streamlit UI -> (voice?) -> AudioRecorder -> Whisper -> text
text -> Memory + Retriever(FAISS+Fireworks) -> Context
Context + History -> Prompt -> Fireworks LLM -> Reply
Reply -> (autoplay?) -> Edge-TTS -> Audio
[Optional] "Get feedback" -> Feedback chain -> report.txt
```
</details>

---

## Prerequisites

- **Python**: 3.11.x recommended  
- **Fireworks API key**: create one in your Fireworks account
- **Optional GPU for STT (Whisper)** ‚Äî tested on HP Victus (RTX 3060)
  - NVIDIA driver (recent)
  - CUDA Toolkit **12.x or 13.x**
  - cuDNN **9.x** (ensure `cudnn_ops64_9.dll` is on `PATH`)
  - _LLM calls run in Fireworks‚Äô cloud; GPU is only for local Whisper._

---

## Install & run

```bash
# 1) Clone
git clone https://github.com/<your-username>/GPTInterviewer.git
cd GPTInterviewer

# 2) Create venv
python -m venv .venv
# Windows:
. .venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

# 3) Install deps
pip install -r requirements.txt

# 4) Configure your key
copy .env.example .env   # Windows (or: cp .env.example .env)
# then edit .env and set FIREWORKS_API_KEY=fw_...

# 5) Run
streamlit run Homepage.py
```

**CUDA Whisper sanity check (PowerShell):**
```powershell
$code = @"
from faster_whisper import WhisperModel
try:
    WhisperModel("small", device="cuda", compute_type="float16")
    print("‚úÖ CUDA Whisper OK")
except Exception as e:
    print("‚ùå CUDA Whisper failed:", e)
"@
python -c $code
```

---

## Environment config

Create `.env` in repo root:

```
FIREWORKS_API_KEY=fw_XXXXXXXXXXXXXXXX
# Optional STT settings (offline.py reads these)
WHISPER_MODEL=small           # tiny/base/small/medium/large-v3
WHISPER_DEVICE=cuda           # cuda or cpu
WHISPER_COMPUTE=float16       # float16 | float32 | int8_float16 | int8
```

---

## Usage

1. Open **Homepage** ‚Üí pick **Professional**, **Behavioral**, or **R√©sum√©**.
2. **Professional**: paste JD ‚Üí toggle voice if you like ‚Üí answer questions.
3. **Behavioral**: enter prompt/keywords ‚Üí answer.
4. **R√©sum√©**: upload PDF ‚Üí pick position ‚Üí answer.
5. Hit **Get feedback** any time to download a scored report.

---

## Performance tips

- **Chunking / retriever**: default overlap is tuned for medium JDs/resumes. If your prompts are huge, reduce chunk size or top‚Äëk in code.
- **Turn cap**: each screen enforces **MAX_TURNS** to keep sessions ~10‚Äì15 min.
- **Voice off**: if your mic is flaky, disable voice to remove STT overhead.

---

## Troubleshooting

| Symptom | Fix |
|---|---|
| `cudnn_ops64_9.dll missing` | Ensure cuDNN `bin` is on PATH. Example (PowerShell): `setx PATH "$env:PATH;C:\Program Files\NVIDIA\CUDA\v13.0\bin;C:\Program Files\NVIDIA\CUDNN\v9.12\bin\13.0"` then open a **new** terminal. |
| `Requested int8_float16 compute type...` | Your backend doesn‚Äôt support that mix. Set `WHISPER_COMPUTE=float16` (GPU) or `float32` (CPU). |
| Fireworks `NOT_FOUND / 404` | Use the **full** model slug from `/models`, e.g. `accounts/fireworks/models/llama-v3p1-8b-instruct`. |
| `Prompt is too long` | Your JD/r√©sum√© is huge. Reduce chunk size/overlap or lower retriever `k`. |
| `ModuleNotFoundError: streamlit_lottie` (or others) | `pip install -r requirements.txt` inside the **activated** `.venv`. |
| Repeating questions | Conversation uses message history + asked‚Äëtopics memory. If you still see repeats, increase MAX_TURNS or clear session state. |
| Git repo too big | Add `.venv/`, `models/**`, `*.dll`, `*.pyd`, `*.bin`, `*.safetensors` to `.gitignore`. Clean history: `git filter-repo --invert-paths --path .venv --path models --path '*.dll' --path '*.pyd' --path '*.bin' --path '*.safetensors'`. |

---

## Project layout

```
GPTInterviewer/
‚îú‚îÄ Homepage.py                # entry screen (no internal hacks)
‚îú‚îÄ pages/
‚îÇ  ‚îú‚îÄ Professional Screen.py  # JD-driven interview
‚îÇ  ‚îú‚îÄ Behavioral Screen.py    # soft-skill interview
‚îÇ  ‚îî‚îÄ Resume Screen.py        # resume-driven interview
‚îú‚îÄ prompts/
‚îÇ  ‚îú‚îÄ prompts.py              # templates
‚îÇ  ‚îî‚îÄ prompt_selector.py      # per-position selectors
‚îú‚îÄ speech_recognition/
‚îÇ  ‚îî‚îÄ offline.py              # faster-whisper wrapper (CPU/GPU)
‚îú‚îÄ tts/
‚îÇ  ‚îî‚îÄ edge_speak.py           # Edge-TTS helper
‚îú‚îÄ app_utils.py               # NLTK bootstrap, misc helpers
‚îú‚îÄ requirements.txt
‚îú‚îÄ .env.example
‚îî‚îÄ .gitignore
```

---

## License

MIT ¬© 2025 Souvik Ghosh
